{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "train_model.py"
      ],
      "metadata": {
        "id": "4Mioaq8i0DWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "import warnings\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "import joblib\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def print_section(title, description=\"\"):\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"{title:^100}\")\n",
        "    if description:\n",
        "        print(f\"{description:^100}\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "def print_subsection(title, description=\"\"):\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(f\"{title}\")\n",
        "    if description:\n",
        "        print(f\"{description}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "# Step 1: Data Loading and Initial Exploration\n",
        "def load_and_explore_data():\n",
        "    print_section(\"STEP 1: DATA LOADING AND EXPLORATION\",\n",
        "                 \"Analyzing the Air Quality dataset structure and characteristics\")\n",
        "\n",
        "    print(\"\\nLoading the Air Quality dataset...\")\n",
        "    df = pd.read_excel('AirQualityUCI.xlsx')\n",
        "\n",
        "    print_subsection(\"1.1 Dataset Overview\", \"Basic information about the dataset\")\n",
        "    print(f\"\\nDataset Dimensions:\")\n",
        "    print(f\"Number of rows (samples): {df.shape[0]:,}\")\n",
        "    print(f\"Number of columns (features): {df.shape[1]}\")\n",
        "\n",
        "    print(\"\\nFirst 5 samples of the dataset:\")\n",
        "    print(df.head().to_string())\n",
        "\n",
        "    print_subsection(\"1.2 Data Types and Missing Values Analysis\",\n",
        "                    \"Understanding data types and checking for missing values\")\n",
        "    print(\"\\nData Types:\")\n",
        "    print(df.dtypes.to_string())\n",
        "\n",
        "    print(\"\\nMissing Values Analysis:\")\n",
        "    missing_stats = df.isnull().sum()\n",
        "    missing_percent = (missing_stats / len(df)) * 100\n",
        "    missing_info = pd.DataFrame({\n",
        "        'Missing Count': missing_stats,\n",
        "        'Missing Percentage': missing_percent\n",
        "    })\n",
        "    print(missing_info.to_string())\n",
        "\n",
        "    print_subsection(\"1.3 Statistical Summary\",\n",
        "                    \"Descriptive statistics of numerical features\")\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    df_corr = df.copy()\n",
        "\n",
        "    # Convert Date and Time columns properly\n",
        "    df_corr['Date'] = pd.to_datetime(df_corr['Date'])\n",
        "    # Extract hour from Time column (assuming it's in HH:MM:SS format)\n",
        "    df_corr['Time'] = pd.to_datetime(df_corr['Time'].astype(str)).dt.hour\n",
        "\n",
        "    # Select only numerical columns for correlation\n",
        "    numerical_cols = df_corr.select_dtypes(include=[np.number]).columns\n",
        "    print(df_corr[numerical_cols].describe().to_string())\n",
        "\n",
        "    print(\"\\nFeature Correlation with CO(GT):\")\n",
        "    correlations = df_corr[numerical_cols].corr()['CO(GT)'].sort_values(ascending=False)\n",
        "    print(correlations.to_string())\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    print_section(\"STEP 2: DATA PREPROCESSING\",\n",
        "                 \"Cleaning and preparing data for model training\")\n",
        "\n",
        "    print_subsection(\"2.1 Missing Values Handling\",\n",
        "                    \"Processing missing values marked as -200\")\n",
        "    initial_rows = len(df)\n",
        "\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.replace(-200, np.nan)\n",
        "\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    df_clean[numerical_cols] = imputer.fit_transform(df_clean[numerical_cols])\n",
        "\n",
        "    print(f\"\\nMissing Values Processing Summary:\")\n",
        "    print(f\"Initial dataset size: {initial_rows:,} rows\")\n",
        "    print(f\"Final clean dataset size: {len(df_clean):,} rows\")\n",
        "    print(f\"Percentage of data retained: {(len(df_clean)/initial_rows)*100:.2f}%\")\n",
        "\n",
        "    print_subsection(\"2.2 Feature Engineering\",\n",
        "                    \"Creating new features from existing data\")\n",
        "\n",
        "    df_clean['DateTime'] = pd.to_datetime(df_clean['Date'].astype(str) + ' ' +\n",
        "                                        df_clean['Time'].astype(str))\n",
        "\n",
        "    df_clean['Hour'] = df_clean['DateTime'].dt.hour\n",
        "    df_clean['DayOfWeek'] = df_clean['DateTime'].dt.dayofweek\n",
        "    df_clean['Month'] = df_clean['DateTime'].dt.month\n",
        "    df_clean['DayOfYear'] = df_clean['DateTime'].dt.dayofyear\n",
        "\n",
        "    df_clean['NOx_NO2_ratio'] = df_clean['NOx(GT)'] / (df_clean['NO2(GT)'] + 1e-6)\n",
        "    df_clean['CO_NMHC_ratio'] = df_clean['PT08.S1(CO)'] / (df_clean['NMHC(GT)'] + 1e-6)\n",
        "\n",
        "    base_features = [\n",
        "        'PT08.S1(CO)', 'PT08.S2(NMHC)', 'PT08.S3(NOx)', 'PT08.S4(NO2)',\n",
        "        'PT08.S5(O3)', 'T', 'RH', 'C6H6(GT)', 'NOx(GT)', 'NO2(GT)', 'NMHC(GT)',\n",
        "        'Hour', 'Month', 'DayOfYear', 'NOx_NO2_ratio', 'CO_NMHC_ratio'\n",
        "    ]\n",
        "\n",
        "    # Save feature names\n",
        "    feature_names = pd.DataFrame({'feature_names': base_features})\n",
        "    feature_names.to_csv('feature_names.csv', index=False)\n",
        "\n",
        "    X = df_clean[base_features]\n",
        "    y = df_clean['CO(GT)']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, base_features\n",
        "\n",
        "# Step 3: Model Training and Evaluation\n",
        "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
        "    print_section(\"STEP 3: MODEL TRAINING AND EVALUATION\",\n",
        "                 \"Training and evaluating traditional machine learning models\")\n",
        "\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print_subsection(f\"3.1 {name} Model\",\n",
        "                        f\"Training and evaluating {name} model\")\n",
        "\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "        print(\"\\nCross-validation Results:\")\n",
        "        print(\"Fold\\tR2 Score\")\n",
        "        print(\"-\"*20)\n",
        "        for fold, score in enumerate(cv_scores, 1):\n",
        "            print(f\"{fold}\\t{score:.4f}\")\n",
        "        print(f\"Mean\\t{cv_scores.mean():.4f}\")\n",
        "        print(f\"Std\\t{cv_scores.std():.4f}\")\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        print(\"\\nTest Set Performance Metrics:\")\n",
        "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "        print(f\"R2 Score: {r2:.4f}\")\n",
        "\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'y_pred': y_pred,\n",
        "            'metrics': {\n",
        "                'RMSE': rmse,\n",
        "                'MAE': mae,\n",
        "                'R2': r2\n",
        "            }\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Step 4: Neural Network Implementation\n",
        "def train_neural_network(X_train, X_test, y_train, y_test):\n",
        "    print_section(\"STEP 4: NEURAL NETWORK IMPLEMENTATION\",\n",
        "                 \"Building and training a deep learning model\")\n",
        "\n",
        "    X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "    y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "    y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(X_train.shape[1],))\n",
        "    x = Dense(256, activation='relu')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    residual = x\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.add([x, residual])\n",
        "\n",
        "    residual = x\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.add([x, residual])\n",
        "\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.001,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=10,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_tf, y_train_tf,\n",
        "        epochs=200,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping, lr_scheduler],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(X_test_tf, batch_size=32).flatten()\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R2 Score: {r2:.4f}\")\n",
        "\n",
        "    return model, y_pred, history\n",
        "\n",
        "def visualize_results(y_test, results, nn_pred, nn_history, selected_features):\n",
        "    print_section(\"STEP 5: RESULTS VISUALIZATION\",\n",
        "                 \"Creating visual representations of model performance\")\n",
        "\n",
        "    # Print model architecture and formulas\n",
        "    print_subsection(\"MODEL ARCHITECTURE AND FORMULAS\",\n",
        "                    \"Detailed explanation of model components and formulas\")\n",
        "\n",
        "    print(\"\\n1. Linear Regression Model:\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"Formula: y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  - y: Target variable (CO(GT))\")\n",
        "    print(\"  - β₀: Intercept\")\n",
        "    print(\"  - β₁ to βₙ: Coefficients for each feature\")\n",
        "    print(\"  - x₁ to xₙ: Input features\")\n",
        "    print(\"  - ε: Error term\")\n",
        "    print(\"\\nOptimization: Minimize Mean Squared Error (MSE)\")\n",
        "    print(\"MSE = (1/n) * Σ(yᵢ - ŷᵢ)²\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  - n: Number of samples\")\n",
        "    print(\"  - yᵢ: Actual value\")\n",
        "    print(\"  - ŷᵢ: Predicted value\")\n",
        "\n",
        "    print(\"\\n2. Random Forest Model:\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"Ensemble of Decision Trees:\")\n",
        "    print(\"Each tree prediction: ŷ = f(x₁, x₂, ..., xₙ)\")\n",
        "    print(\"Final prediction: ŷ = (1/T) * Σ ŷᵢ\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  - T: Number of trees (n_estimators=100)\")\n",
        "    print(\"  - ŷᵢ: Prediction from i-th tree\")\n",
        "    print(\"\\nFeature Importance Calculation:\")\n",
        "    print(\"Importance = (1/T) * Σ (node_impurity - left_impurity - right_impurity)\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  - node_impurity: Gini impurity of the node\")\n",
        "    print(\"  - left_impurity: Gini impurity of left child\")\n",
        "    print(\"  - right_impurity: Gini impurity of right child\")\n",
        "\n",
        "    print(\"\\n3. Neural Network Model:\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"Architecture:\")\n",
        "    print(\"Input Layer (16 features) → Dense(256) → BatchNorm → Dropout(0.4)\")\n",
        "    print(\"→ Dense(128) → BatchNorm → Dropout(0.3) → Dense(256) → BatchNorm\")\n",
        "    print(\"→ Dense(64) → BatchNorm → Dropout(0.2) → Dense(256) → BatchNorm\")\n",
        "    print(\"→ Dense(32) → BatchNorm → Output(1)\")\n",
        "\n",
        "    print(\"\\nActivation Functions:\")\n",
        "    print(\"ReLU: f(x) = max(0, x)\")\n",
        "    print(\"Linear: f(x) = x (output layer)\")\n",
        "\n",
        "    print(\"\\nBatch Normalization:\")\n",
        "    print(\"x̂ = (x - μ) / √(σ² + ε)\")\n",
        "    print(\"y = γx̂ + β\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  - μ: Mean of batch\")\n",
        "    print(\"  - σ²: Variance of batch\")\n",
        "    print(\"  - ε: Small constant\")\n",
        "    print(\"  - γ: Scale parameter\")\n",
        "    print(\"  - β: Shift parameter\")\n",
        "\n",
        "    print(\"\\nDropout:\")\n",
        "    print(\"During training: x' = x * m\")\n",
        "    print(\"During inference: x' = x * p\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  - m: Binary mask (Bernoulli distribution)\")\n",
        "    print(\"  - p: Dropout probability\")\n",
        "\n",
        "    print(\"\\n4. Performance Metrics:\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"Root Mean Squared Error (RMSE):\")\n",
        "    print(\"RMSE = √(MSE) = √[(1/n) * Σ(yᵢ - ŷᵢ)²]\")\n",
        "\n",
        "    print(\"\\nMean Absolute Error (MAE):\")\n",
        "    print(\"MAE = (1/n) * Σ|yᵢ - ŷᵢ|\")\n",
        "\n",
        "    print(\"\\nR² Score (Coefficient of Determination):\")\n",
        "    print(\"R² = 1 - (Σ(yᵢ - ŷᵢ)² / Σ(yᵢ - ȳ)²)\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  - ȳ: Mean of actual values\")\n",
        "\n",
        "    print(\"\\n5. Error Distribution Metrics:\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"Skewness:\")\n",
        "    print(\"γ₁ = (1/n) * Σ[(xᵢ - μ)/σ]³\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  - μ: Mean of errors\")\n",
        "    print(\"  - σ: Standard deviation of errors\")\n",
        "\n",
        "    print(\"\\nKurtosis:\")\n",
        "    print(\"γ₂ = (1/n) * Σ[(xᵢ - μ)/σ]⁴ - 3\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  - μ: Mean of errors\")\n",
        "    print(\"  - σ: Standard deviation of errors\")\n",
        "\n",
        "    # Create a directory for saving plots\n",
        "    os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "    print_subsection(\"5.1 Model Performance Comparison\",\n",
        "                    \"Comparing predictions across all models\")\n",
        "\n",
        "    # Calculate and print detailed statistics for each model\n",
        "    print(\"\\nDetailed Model Performance Statistics:\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"{'Model':<20} {'RMSE':<10} {'MAE':<10} {'R2':<10} {'Max Error':<12} {'Min Error':<12}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for name, result in results.items():\n",
        "        errors = y_test - result['y_pred']\n",
        "        print(f\"{name:<20} {result['metrics']['RMSE']:.4f} {result['metrics']['MAE']:.4f} \"\n",
        "              f\"{result['metrics']['R2']:.4f} {errors.max():.4f} {errors.min():.4f}\")\n",
        "\n",
        "    # Neural Network statistics\n",
        "    nn_errors = y_test - nn_pred\n",
        "    nn_mse = mean_squared_error(y_test, nn_pred)\n",
        "    nn_rmse = np.sqrt(nn_mse)\n",
        "    nn_mae = mean_absolute_error(y_test, nn_pred)\n",
        "    nn_r2 = r2_score(y_test, nn_pred)\n",
        "\n",
        "    print(f\"{'Neural Network':<20} {nn_rmse:.4f} {nn_mae:.4f} {nn_r2:.4f} \"\n",
        "          f\"{nn_errors.max():.4f} {nn_errors.min():.4f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Plot actual vs predicted for all models\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot for traditional models\n",
        "    for i, (name, result) in enumerate(results.items(), 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        plt.scatter(y_test, result['y_pred'], alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "        plt.xlabel('Actual Values')\n",
        "        plt.ylabel('Predicted Values')\n",
        "        plt.title(f'{name} - Actual vs Predicted\\nR2: {result[\"metrics\"][\"R2\"]:.4f}')\n",
        "        plt.grid(True)\n",
        "\n",
        "    # Plot for Neural Network\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.scatter(y_test, nn_pred, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Neural Network - Actual vs Predicted\\nR2: {nn_r2:.4f}')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(nn_history.history['loss'], label='Training Loss')\n",
        "    plt.plot(nn_history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Neural Network Training History\\nFinal Loss: {:.4f}'.format(nn_history.history['loss'][-1]))\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/model_performance.png')\n",
        "    plt.close()\n",
        "\n",
        "    print_subsection(\"5.2 Feature Importance Analysis\",\n",
        "                    \"Analyzing feature importance in Random Forest model\")\n",
        "\n",
        "    # Plot feature importance for Random Forest\n",
        "    rf_model = results['Random Forest']['model']\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': selected_features,\n",
        "        'Importance': rf_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 5 Most Important Features:\")\n",
        "    print(\"-\"*50)\n",
        "    for i, (feature, importance) in enumerate(zip(feature_importance['Feature'], feature_importance['Importance']), 1):\n",
        "        print(f\"{i}. {feature:<20} {importance:.4f}\")\n",
        "        if i == 5:\n",
        "            break\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "    plt.title('Feature Importance (Random Forest)\\nTotal Features: {}'.format(len(selected_features)))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/feature_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "    print_subsection(\"5.3 Error Distribution Analysis\",\n",
        "                    \"Analyzing error distribution across models\")\n",
        "\n",
        "    # Calculate errors for each model\n",
        "    errors = {}\n",
        "    for name, result in results.items():\n",
        "        errors[name] = y_test - result['y_pred']\n",
        "    errors['Neural Network'] = y_test - nn_pred\n",
        "\n",
        "    print(\"\\nError Distribution Statistics:\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"{'Model':<20} {'Mean Error':<12} {'Std Error':<12} {'Skewness':<12} {'Kurtosis':<12}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for name, error in errors.items():\n",
        "        print(f\"{name:<20} {error.mean():.4f} {error.std():.4f} \"\n",
        "              f\"{pd.Series(error).skew():.4f} {pd.Series(error).kurtosis():.4f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Plot error distributions\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, (name, error) in enumerate(errors.items(), 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        sns.histplot(error, kde=True)\n",
        "        plt.xlabel('Prediction Error')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(f'{name} Error Distribution\\nMean: {error.mean():.4f}, Std: {error.std():.4f}')\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/error_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    print_subsection(\"5.4 Model Performance Metrics\",\n",
        "                    \"Comparing model performance metrics\")\n",
        "\n",
        "    # Create a DataFrame of metrics\n",
        "    metrics_data = []\n",
        "    for name, result in results.items():\n",
        "        metrics_data.append({\n",
        "            'Model': name,\n",
        "            'RMSE': result['metrics']['RMSE'],\n",
        "            'MAE': result['metrics']['MAE'],\n",
        "            'R2': result['metrics']['R2']\n",
        "        })\n",
        "\n",
        "    metrics_data.append({\n",
        "        'Model': 'Neural Network',\n",
        "        'RMSE': nn_rmse,\n",
        "        'MAE': nn_mae,\n",
        "        'R2': nn_r2\n",
        "    })\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "    print(\"\\nDetailed Model Performance Metrics:\")\n",
        "    print(\"-\"*80)\n",
        "    print(metrics_df.to_string(index=False))\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Plot metrics\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    metrics = ['RMSE', 'MAE', 'R2']\n",
        "    for i, metric in enumerate(metrics, 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        sns.barplot(x='Model', y=metric, data=metrics_df)\n",
        "        plt.title(f'{metric} Comparison\\nBest: {metrics_df[metric].min():.4f}')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/metrics_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nVisualization Summary:\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"1. Model Performance Comparison (model_performance.png):\")\n",
        "    print(\"   - Actual vs Predicted scatter plots for all models\")\n",
        "    print(\"   - Neural Network training history\")\n",
        "    print(\"   - R2 scores for each model\")\n",
        "    print(\"   - Formulas used for each model's predictions\")\n",
        "\n",
        "    print(\"\\n2. Feature Importance Analysis (feature_importance.png):\")\n",
        "    print(\"   - Bar plot of feature importance from Random Forest\")\n",
        "    print(\"   - Top 5 most important features identified\")\n",
        "    print(\"   - Feature importance calculation formula\")\n",
        "\n",
        "    print(\"\\n3. Error Distribution Analysis (error_distribution.png):\")\n",
        "    print(\"   - Histograms of prediction errors for each model\")\n",
        "    print(\"   - Error statistics including mean, std, skewness, and kurtosis\")\n",
        "    print(\"   - Error distribution formulas and calculations\")\n",
        "\n",
        "    print(\"\\n4. Model Performance Metrics (metrics_comparison.png):\")\n",
        "    print(\"   - Comparison of RMSE, MAE, and R2 across all models\")\n",
        "    print(\"   - Best performing model for each metric\")\n",
        "    print(\"   - Detailed formulas for each metric\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nAll visualizations have been saved in the 'plots' directory.\")\n",
        "    print(\"You can find detailed performance metrics, formulas, and analysis in the plots.\")\n",
        "\n",
        "def main():\n",
        "    print_section(\"AIR QUALITY INDEX PREDICTION PROJECT\",\n",
        "                 \"Predicting CO levels using multiple machine learning models\")\n",
        "\n",
        "    df = load_and_explore_data()\n",
        "    X_train, X_test, y_train, y_test, scaler, selected_features = preprocess_data(df)\n",
        "    results = train_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
        "    nn_model, nn_pred, nn_history = train_neural_network(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Add visualization step\n",
        "    visualize_results(y_test, results, nn_pred, nn_history, selected_features)\n",
        "\n",
        "    # Save models and scaler\n",
        "    print_section(\"SAVING MODELS\",\n",
        "                 \"Saving trained models and scaler for future predictions\")\n",
        "\n",
        "    try:\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        joblib.dump(results['Linear Regression']['model'], 'models/linear_model.joblib')\n",
        "        joblib.dump(results['Random Forest']['model'], 'models/rf_model.joblib')\n",
        "        nn_model.save('models/nn_model.keras')\n",
        "        joblib.dump(scaler, 'models/scaler.joblib')\n",
        "        print(\"\\nModels and scaler saved successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving models: {e}\")\n",
        "\n",
        "    print_section(\"PROJECT COMPLETED\",\n",
        "                 \"All models have been trained and evaluated successfully\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zenxjwIi0JZT",
        "outputId": "574197b1-d80b-4c3a-a197-385697b38c81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "                                AIR QUALITY INDEX PREDICTION PROJECT                                \n",
            "                    Predicting CO levels using multiple machine learning models                     \n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "                                STEP 1: DATA LOADING AND EXPLORATION                                \n",
            "                  Analyzing the Air Quality dataset structure and characteristics                   \n",
            "====================================================================================================\n",
            "\n",
            "Loading the Air Quality dataset...\n",
            "\n",
            "--------------------------------------------------\n",
            "1.1 Dataset Overview\n",
            "Basic information about the dataset\n",
            "--------------------------------------------------\n",
            "\n",
            "Dataset Dimensions:\n",
            "Number of rows (samples): 9,357\n",
            "Number of columns (features): 15\n",
            "\n",
            "First 5 samples of the dataset:\n",
            "        Date      Time  CO(GT)  PT08.S1(CO)  NMHC(GT)   C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)      T         RH        AH\n",
            "0 2004-03-10  18:00:00     2.6      1360.00       150  11.881723        1045.50    166.0       1056.25    113.0       1692.00      1267.50  13.60  48.875001  0.757754\n",
            "1 2004-03-10  19:00:00     2.0      1292.25       112   9.397165         954.75    103.0       1173.75     92.0       1558.75       972.25  13.30  47.700000  0.725487\n",
            "2 2004-03-10  20:00:00     2.2      1402.00        88   8.997817         939.25    131.0       1140.00    114.0       1554.50      1074.00  11.90  53.975000  0.750239\n",
            "3 2004-03-10  21:00:00     2.2      1375.50        80   9.228796         948.25    172.0       1092.00    122.0       1583.75      1203.25  11.00  60.000000  0.786713\n",
            "4 2004-03-10  22:00:00     1.6      1272.25        51   6.518224         835.50    131.0       1205.00    116.0       1490.00      1110.00  11.15  59.575001  0.788794\n",
            "\n",
            "--------------------------------------------------\n",
            "1.2 Data Types and Missing Values Analysis\n",
            "Understanding data types and checking for missing values\n",
            "--------------------------------------------------\n",
            "\n",
            "Data Types:\n",
            "Date             datetime64[ns]\n",
            "Time                     object\n",
            "CO(GT)                  float64\n",
            "PT08.S1(CO)             float64\n",
            "NMHC(GT)                  int64\n",
            "C6H6(GT)                float64\n",
            "PT08.S2(NMHC)           float64\n",
            "NOx(GT)                 float64\n",
            "PT08.S3(NOx)            float64\n",
            "NO2(GT)                 float64\n",
            "PT08.S4(NO2)            float64\n",
            "PT08.S5(O3)             float64\n",
            "T                       float64\n",
            "RH                      float64\n",
            "AH                      float64\n",
            "\n",
            "Missing Values Analysis:\n",
            "               Missing Count  Missing Percentage\n",
            "Date                       0                 0.0\n",
            "Time                       0                 0.0\n",
            "CO(GT)                     0                 0.0\n",
            "PT08.S1(CO)                0                 0.0\n",
            "NMHC(GT)                   0                 0.0\n",
            "C6H6(GT)                   0                 0.0\n",
            "PT08.S2(NMHC)              0                 0.0\n",
            "NOx(GT)                    0                 0.0\n",
            "PT08.S3(NOx)               0                 0.0\n",
            "NO2(GT)                    0                 0.0\n",
            "PT08.S4(NO2)               0                 0.0\n",
            "PT08.S5(O3)                0                 0.0\n",
            "T                          0                 0.0\n",
            "RH                         0                 0.0\n",
            "AH                         0                 0.0\n",
            "\n",
            "--------------------------------------------------\n",
            "1.3 Statistical Summary\n",
            "Descriptive statistics of numerical features\n",
            "--------------------------------------------------\n",
            "\n",
            "Basic Statistics:\n",
            "              Time       CO(GT)  PT08.S1(CO)     NMHC(GT)     C6H6(GT)  PT08.S2(NMHC)      NOx(GT)  PT08.S3(NOx)      NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)            T           RH           AH\n",
            "count  9357.000000  9357.000000  9357.000000  9357.000000  9357.000000    9357.000000  9357.000000   9357.000000  9357.000000   9357.000000  9357.000000  9357.000000  9357.000000  9357.000000\n",
            "mean     11.498557   -34.207524  1048.869652  -159.090093     1.865576     894.475963   168.604200    794.872333    58.135898   1391.363266   974.951534     9.776600    39.483611    -6.837604\n",
            "std       6.923182    77.657170   329.817015   139.789093    41.380154     342.315902   257.424561    321.977031   126.931428    467.192382   456.922728    43.203438    51.215645    38.976670\n",
            "min       0.000000  -200.000000  -200.000000  -200.000000  -200.000000    -200.000000  -200.000000   -200.000000  -200.000000   -200.000000  -200.000000  -200.000000  -200.000000  -200.000000\n",
            "25%       5.000000     0.600000   921.000000  -200.000000     4.004958     711.000000    50.000000    637.000000    53.000000   1184.750000   699.750000    10.950000    34.050000     0.692275\n",
            "50%      11.000000     1.500000  1052.500000  -200.000000     7.886653     894.500000   141.000000    794.250000    96.000000   1445.500000   942.000000    17.200000    48.550000     0.976823\n",
            "75%      18.000000     2.600000  1221.250000  -200.000000    13.636091    1104.750000   284.200000    960.250000   133.000000   1662.000000  1255.250000    24.075000    61.875000     1.296223\n",
            "max      23.000000    11.900000  2039.750000  1189.000000    63.741476    2214.000000  1479.000000   2682.750000   339.700000   2775.000000  2522.750000    44.600000    88.725000     2.231036\n",
            "\n",
            "Feature Correlation with CO(GT):\n",
            "CO(GT)           1.000000\n",
            "NO2(GT)          0.671140\n",
            "NOx(GT)          0.526450\n",
            "NMHC(GT)         0.128351\n",
            "PT08.S5(O3)      0.080316\n",
            "Time             0.075984\n",
            "PT08.S1(CO)      0.041415\n",
            "PT08.S2(NMHC)    0.029939\n",
            "C6H6(GT)        -0.031377\n",
            "AH              -0.045892\n",
            "RH              -0.048231\n",
            "T               -0.068952\n",
            "PT08.S4(NO2)    -0.073721\n",
            "PT08.S3(NOx)    -0.089981\n",
            "\n",
            "====================================================================================================\n",
            "                                     STEP 2: DATA PREPROCESSING                                     \n",
            "                           Cleaning and preparing data for model training                           \n",
            "====================================================================================================\n",
            "\n",
            "--------------------------------------------------\n",
            "2.1 Missing Values Handling\n",
            "Processing missing values marked as -200\n",
            "--------------------------------------------------\n",
            "\n",
            "Missing Values Processing Summary:\n",
            "Initial dataset size: 9,357 rows\n",
            "Final clean dataset size: 9,357 rows\n",
            "Percentage of data retained: 100.00%\n",
            "\n",
            "--------------------------------------------------\n",
            "2.2 Feature Engineering\n",
            "Creating new features from existing data\n",
            "--------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "                               STEP 3: MODEL TRAINING AND EVALUATION                                \n",
            "                    Training and evaluating traditional machine learning models                     \n",
            "====================================================================================================\n",
            "\n",
            "--------------------------------------------------\n",
            "3.1 Linear Regression Model\n",
            "Training and evaluating Linear Regression model\n",
            "--------------------------------------------------\n",
            "\n",
            "Cross-validation Results:\n",
            "Fold\tR2 Score\n",
            "--------------------\n",
            "1\t0.9118\n",
            "2\t0.9098\n",
            "3\t0.9092\n",
            "4\t0.9122\n",
            "5\t0.9210\n",
            "Mean\t0.9128\n",
            "Std\t0.0043\n",
            "\n",
            "Test Set Performance Metrics:\n",
            "Root Mean Squared Error (RMSE): 0.4125\n",
            "Mean Absolute Error (MAE): 0.2603\n",
            "R2 Score: 0.9206\n",
            "\n",
            "--------------------------------------------------\n",
            "3.1 Random Forest Model\n",
            "Training and evaluating Random Forest model\n",
            "--------------------------------------------------\n",
            "\n",
            "Cross-validation Results:\n",
            "Fold\tR2 Score\n",
            "--------------------\n",
            "1\t0.9342\n",
            "2\t0.9428\n",
            "3\t0.9319\n",
            "4\t0.9345\n",
            "5\t0.9404\n",
            "Mean\t0.9368\n",
            "Std\t0.0041\n",
            "\n",
            "Test Set Performance Metrics:\n",
            "Root Mean Squared Error (RMSE): 0.3481\n",
            "Mean Absolute Error (MAE): 0.2135\n",
            "R2 Score: 0.9435\n",
            "\n",
            "====================================================================================================\n",
            "                               STEP 4: NEURAL NETWORK IMPLEMENTATION                                \n",
            "                            Building and training a deep learning model                             \n",
            "====================================================================================================\n",
            "Epoch 1/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 4.3662 - mae: 1.7963 - val_loss: 1.2012 - val_mae: 0.8077 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6726 - mae: 0.6242 - val_loss: 0.2190 - val_mae: 0.3476 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3520 - mae: 0.4343 - val_loss: 0.1795 - val_mae: 0.2914 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3135 - mae: 0.3964 - val_loss: 0.1553 - val_mae: 0.2791 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2919 - mae: 0.3903 - val_loss: 0.1519 - val_mae: 0.2719 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2907 - mae: 0.3862 - val_loss: 0.1531 - val_mae: 0.2668 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2935 - mae: 0.3882 - val_loss: 0.1480 - val_mae: 0.2627 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2649 - mae: 0.3649 - val_loss: 0.1427 - val_mae: 0.2590 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.2617 - mae: 0.3631 - val_loss: 0.1496 - val_mae: 0.2688 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2562 - mae: 0.3559 - val_loss: 0.1374 - val_mae: 0.2489 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2740 - mae: 0.3632 - val_loss: 0.1391 - val_mae: 0.2531 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2625 - mae: 0.3592 - val_loss: 0.1432 - val_mae: 0.2559 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2438 - mae: 0.3456 - val_loss: 0.1459 - val_mae: 0.2590 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2618 - mae: 0.3587 - val_loss: 0.1342 - val_mae: 0.2444 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2329 - mae: 0.3496 - val_loss: 0.1360 - val_mae: 0.2450 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2427 - mae: 0.3554 - val_loss: 0.1475 - val_mae: 0.2661 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2417 - mae: 0.3598 - val_loss: 0.1364 - val_mae: 0.2529 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.2285 - mae: 0.3401 - val_loss: 0.1490 - val_mae: 0.2600 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2170 - mae: 0.3324 - val_loss: 0.1375 - val_mae: 0.2476 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2495 - mae: 0.3584 - val_loss: 0.1532 - val_mae: 0.2755 - learning_rate: 0.0010\n",
            "Epoch 21/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2307 - mae: 0.3430 - val_loss: 0.1528 - val_mae: 0.2779 - learning_rate: 0.0010\n",
            "Epoch 22/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2431 - mae: 0.3561 - val_loss: 0.1508 - val_mae: 0.2591 - learning_rate: 0.0010\n",
            "Epoch 23/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2315 - mae: 0.3510 - val_loss: 0.1344 - val_mae: 0.2446 - learning_rate: 0.0010\n",
            "Epoch 24/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2112 - mae: 0.3271\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2112 - mae: 0.3271 - val_loss: 0.1350 - val_mae: 0.2433 - learning_rate: 0.0010\n",
            "Epoch 25/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2354 - mae: 0.3426 - val_loss: 0.1275 - val_mae: 0.2377 - learning_rate: 5.0000e-04\n",
            "Epoch 26/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2173 - mae: 0.3329 - val_loss: 0.1289 - val_mae: 0.2380 - learning_rate: 5.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.2018 - mae: 0.3204 - val_loss: 0.1256 - val_mae: 0.2357 - learning_rate: 5.0000e-04\n",
            "Epoch 28/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2388 - mae: 0.3537 - val_loss: 0.1345 - val_mae: 0.2419 - learning_rate: 5.0000e-04\n",
            "Epoch 29/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.2045 - mae: 0.3194 - val_loss: 0.1307 - val_mae: 0.2410 - learning_rate: 5.0000e-04\n",
            "Epoch 30/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2236 - mae: 0.3385 - val_loss: 0.1372 - val_mae: 0.2483 - learning_rate: 5.0000e-04\n",
            "Epoch 31/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1896 - mae: 0.3104 - val_loss: 0.1273 - val_mae: 0.2365 - learning_rate: 5.0000e-04\n",
            "Epoch 32/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2233 - mae: 0.3380 - val_loss: 0.1297 - val_mae: 0.2373 - learning_rate: 5.0000e-04\n",
            "Epoch 33/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1987 - mae: 0.3226 - val_loss: 0.1267 - val_mae: 0.2313 - learning_rate: 5.0000e-04\n",
            "Epoch 34/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2198 - mae: 0.3395 - val_loss: 0.1292 - val_mae: 0.2352 - learning_rate: 5.0000e-04\n",
            "Epoch 35/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.2011 - mae: 0.3299 - val_loss: 0.1243 - val_mae: 0.2306 - learning_rate: 5.0000e-04\n",
            "Epoch 36/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1998 - mae: 0.3223 - val_loss: 0.1241 - val_mae: 0.2294 - learning_rate: 5.0000e-04\n",
            "Epoch 37/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.2218 - mae: 0.3360 - val_loss: 0.1253 - val_mae: 0.2335 - learning_rate: 5.0000e-04\n",
            "Epoch 38/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2146 - mae: 0.3372 - val_loss: 0.1306 - val_mae: 0.2414 - learning_rate: 5.0000e-04\n",
            "Epoch 39/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2249 - mae: 0.3388 - val_loss: 0.1283 - val_mae: 0.2424 - learning_rate: 5.0000e-04\n",
            "Epoch 40/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2447 - mae: 0.3585 - val_loss: 0.1292 - val_mae: 0.2341 - learning_rate: 5.0000e-04\n",
            "Epoch 41/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2055 - mae: 0.3293 - val_loss: 0.1339 - val_mae: 0.2449 - learning_rate: 5.0000e-04\n",
            "Epoch 42/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1998 - mae: 0.3216 - val_loss: 0.1338 - val_mae: 0.2451 - learning_rate: 5.0000e-04\n",
            "Epoch 43/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.2076 - mae: 0.3320 - val_loss: 0.1324 - val_mae: 0.2439 - learning_rate: 5.0000e-04\n",
            "Epoch 44/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2040 - mae: 0.3213 - val_loss: 0.1334 - val_mae: 0.2429 - learning_rate: 5.0000e-04\n",
            "Epoch 45/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1921 - mae: 0.3158 - val_loss: 0.1282 - val_mae: 0.2355 - learning_rate: 5.0000e-04\n",
            "Epoch 46/200\n",
            "\u001b[1m180/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2063 - mae: 0.3303\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2063 - mae: 0.3303 - val_loss: 0.1308 - val_mae: 0.2429 - learning_rate: 5.0000e-04\n",
            "Epoch 47/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2069 - mae: 0.3332 - val_loss: 0.1309 - val_mae: 0.2412 - learning_rate: 2.5000e-04\n",
            "Epoch 48/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2044 - mae: 0.3302 - val_loss: 0.1245 - val_mae: 0.2301 - learning_rate: 2.5000e-04\n",
            "Epoch 49/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1875 - mae: 0.3096 - val_loss: 0.1235 - val_mae: 0.2304 - learning_rate: 2.5000e-04\n",
            "Epoch 50/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2145 - mae: 0.3325 - val_loss: 0.1225 - val_mae: 0.2277 - learning_rate: 2.5000e-04\n",
            "Epoch 51/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2026 - mae: 0.3262 - val_loss: 0.1264 - val_mae: 0.2304 - learning_rate: 2.5000e-04\n",
            "Epoch 52/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1947 - mae: 0.3185 - val_loss: 0.1270 - val_mae: 0.2323 - learning_rate: 2.5000e-04\n",
            "Epoch 53/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2094 - mae: 0.3399 - val_loss: 0.1245 - val_mae: 0.2299 - learning_rate: 2.5000e-04\n",
            "Epoch 54/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1890 - mae: 0.3133 - val_loss: 0.1291 - val_mae: 0.2368 - learning_rate: 2.5000e-04\n",
            "Epoch 55/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2104 - mae: 0.3260 - val_loss: 0.1234 - val_mae: 0.2281 - learning_rate: 2.5000e-04\n",
            "Epoch 56/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1979 - mae: 0.3253 - val_loss: 0.1226 - val_mae: 0.2253 - learning_rate: 2.5000e-04\n",
            "Epoch 57/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.1994 - mae: 0.3269 - val_loss: 0.1290 - val_mae: 0.2334 - learning_rate: 2.5000e-04\n",
            "Epoch 58/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1987 - mae: 0.3287 - val_loss: 0.1247 - val_mae: 0.2274 - learning_rate: 2.5000e-04\n",
            "Epoch 59/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2037 - mae: 0.3269 - val_loss: 0.1222 - val_mae: 0.2270 - learning_rate: 2.5000e-04\n",
            "Epoch 60/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1855 - mae: 0.3122 - val_loss: 0.1219 - val_mae: 0.2251 - learning_rate: 2.5000e-04\n",
            "Epoch 61/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1961 - mae: 0.3150 - val_loss: 0.1240 - val_mae: 0.2283 - learning_rate: 2.5000e-04\n",
            "Epoch 62/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1766 - mae: 0.3077 - val_loss: 0.1238 - val_mae: 0.2327 - learning_rate: 2.5000e-04\n",
            "Epoch 63/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1835 - mae: 0.3111 - val_loss: 0.1226 - val_mae: 0.2263 - learning_rate: 2.5000e-04\n",
            "Epoch 64/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1727 - mae: 0.3002 - val_loss: 0.1232 - val_mae: 0.2258 - learning_rate: 2.5000e-04\n",
            "Epoch 65/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2000 - mae: 0.3259 - val_loss: 0.1252 - val_mae: 0.2289 - learning_rate: 2.5000e-04\n",
            "Epoch 66/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1972 - mae: 0.3287 - val_loss: 0.1214 - val_mae: 0.2258 - learning_rate: 2.5000e-04\n",
            "Epoch 67/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1815 - mae: 0.3155 - val_loss: 0.1232 - val_mae: 0.2290 - learning_rate: 2.5000e-04\n",
            "Epoch 68/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2028 - mae: 0.3335 - val_loss: 0.1242 - val_mae: 0.2281 - learning_rate: 2.5000e-04\n",
            "Epoch 69/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1946 - mae: 0.3226 - val_loss: 0.1286 - val_mae: 0.2322 - learning_rate: 2.5000e-04\n",
            "Epoch 70/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1921 - mae: 0.3174 - val_loss: 0.1209 - val_mae: 0.2234 - learning_rate: 2.5000e-04\n",
            "Epoch 71/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1884 - mae: 0.3151 - val_loss: 0.1224 - val_mae: 0.2258 - learning_rate: 2.5000e-04\n",
            "Epoch 72/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1834 - mae: 0.3167 - val_loss: 0.1219 - val_mae: 0.2238 - learning_rate: 2.5000e-04\n",
            "Epoch 73/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.1954 - mae: 0.3252 - val_loss: 0.1217 - val_mae: 0.2255 - learning_rate: 2.5000e-04\n",
            "Epoch 74/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.1785 - mae: 0.3063 - val_loss: 0.1204 - val_mae: 0.2245 - learning_rate: 2.5000e-04\n",
            "Epoch 75/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1881 - mae: 0.3218 - val_loss: 0.1216 - val_mae: 0.2234 - learning_rate: 2.5000e-04\n",
            "Epoch 76/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1918 - mae: 0.3218 - val_loss: 0.1217 - val_mae: 0.2253 - learning_rate: 2.5000e-04\n",
            "Epoch 77/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1789 - mae: 0.3074 - val_loss: 0.1240 - val_mae: 0.2252 - learning_rate: 2.5000e-04\n",
            "Epoch 78/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1831 - mae: 0.3126 - val_loss: 0.1221 - val_mae: 0.2290 - learning_rate: 2.5000e-04\n",
            "Epoch 79/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1719 - mae: 0.3036 - val_loss: 0.1228 - val_mae: 0.2267 - learning_rate: 2.5000e-04\n",
            "Epoch 80/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1829 - mae: 0.3111 - val_loss: 0.1203 - val_mae: 0.2221 - learning_rate: 2.5000e-04\n",
            "Epoch 81/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.2011 - mae: 0.3244 - val_loss: 0.1215 - val_mae: 0.2237 - learning_rate: 2.5000e-04\n",
            "Epoch 82/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1805 - mae: 0.3117 - val_loss: 0.1211 - val_mae: 0.2263 - learning_rate: 2.5000e-04\n",
            "Epoch 83/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1741 - mae: 0.2989 - val_loss: 0.1222 - val_mae: 0.2285 - learning_rate: 2.5000e-04\n",
            "Epoch 84/200\n",
            "\u001b[1m183/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2046 - mae: 0.3312\n",
            "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.2041 - mae: 0.3308 - val_loss: 0.1235 - val_mae: 0.2285 - learning_rate: 2.5000e-04\n",
            "Epoch 85/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1964 - mae: 0.3236 - val_loss: 0.1228 - val_mae: 0.2278 - learning_rate: 1.2500e-04\n",
            "Epoch 86/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.1773 - mae: 0.3100 - val_loss: 0.1222 - val_mae: 0.2267 - learning_rate: 1.2500e-04\n",
            "Epoch 87/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1898 - mae: 0.3224 - val_loss: 0.1207 - val_mae: 0.2244 - learning_rate: 1.2500e-04\n",
            "Epoch 88/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1843 - mae: 0.3195 - val_loss: 0.1211 - val_mae: 0.2260 - learning_rate: 1.2500e-04\n",
            "Epoch 89/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1777 - mae: 0.3098 - val_loss: 0.1194 - val_mae: 0.2219 - learning_rate: 1.2500e-04\n",
            "Epoch 90/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1766 - mae: 0.3113 - val_loss: 0.1219 - val_mae: 0.2280 - learning_rate: 1.2500e-04\n",
            "Epoch 91/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1957 - mae: 0.3231 - val_loss: 0.1207 - val_mae: 0.2252 - learning_rate: 1.2500e-04\n",
            "Epoch 92/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1829 - mae: 0.3117 - val_loss: 0.1183 - val_mae: 0.2241 - learning_rate: 1.2500e-04\n",
            "Epoch 93/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1892 - mae: 0.3222 - val_loss: 0.1184 - val_mae: 0.2223 - learning_rate: 1.2500e-04\n",
            "Epoch 94/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.2066 - mae: 0.3464 - val_loss: 0.1166 - val_mae: 0.2224 - learning_rate: 1.2500e-04\n",
            "Epoch 95/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1895 - mae: 0.3224 - val_loss: 0.1178 - val_mae: 0.2233 - learning_rate: 1.2500e-04\n",
            "Epoch 96/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1804 - mae: 0.3107 - val_loss: 0.1197 - val_mae: 0.2264 - learning_rate: 1.2500e-04\n",
            "Epoch 97/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1828 - mae: 0.3216 - val_loss: 0.1190 - val_mae: 0.2233 - learning_rate: 1.2500e-04\n",
            "Epoch 98/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1720 - mae: 0.3027 - val_loss: 0.1183 - val_mae: 0.2234 - learning_rate: 1.2500e-04\n",
            "Epoch 99/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1911 - mae: 0.3244 - val_loss: 0.1190 - val_mae: 0.2272 - learning_rate: 1.2500e-04\n",
            "Epoch 100/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1954 - mae: 0.3253 - val_loss: 0.1191 - val_mae: 0.2237 - learning_rate: 1.2500e-04\n",
            "Epoch 101/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1861 - mae: 0.3173 - val_loss: 0.1204 - val_mae: 0.2274 - learning_rate: 1.2500e-04\n",
            "Epoch 102/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.1694 - mae: 0.2988 - val_loss: 0.1181 - val_mae: 0.2226 - learning_rate: 1.2500e-04\n",
            "Epoch 103/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1652 - mae: 0.2975 - val_loss: 0.1181 - val_mae: 0.2242 - learning_rate: 1.2500e-04\n",
            "Epoch 104/200\n",
            "\u001b[1m185/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1724 - mae: 0.3104\n",
            "Epoch 104: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1724 - mae: 0.3103 - val_loss: 0.1172 - val_mae: 0.2219 - learning_rate: 1.2500e-04\n",
            "Epoch 105/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1777 - mae: 0.3086 - val_loss: 0.1191 - val_mae: 0.2261 - learning_rate: 6.2500e-05\n",
            "Epoch 106/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1668 - mae: 0.2965 - val_loss: 0.1172 - val_mae: 0.2206 - learning_rate: 6.2500e-05\n",
            "Epoch 107/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1920 - mae: 0.3213 - val_loss: 0.1160 - val_mae: 0.2200 - learning_rate: 6.2500e-05\n",
            "Epoch 108/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1642 - mae: 0.2946 - val_loss: 0.1167 - val_mae: 0.2208 - learning_rate: 6.2500e-05\n",
            "Epoch 109/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1742 - mae: 0.3036 - val_loss: 0.1161 - val_mae: 0.2208 - learning_rate: 6.2500e-05\n",
            "Epoch 110/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.1800 - mae: 0.3188 - val_loss: 0.1172 - val_mae: 0.2222 - learning_rate: 6.2500e-05\n",
            "Epoch 111/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1839 - mae: 0.3155 - val_loss: 0.1170 - val_mae: 0.2212 - learning_rate: 6.2500e-05\n",
            "Epoch 112/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1692 - mae: 0.3026 - val_loss: 0.1177 - val_mae: 0.2218 - learning_rate: 6.2500e-05\n",
            "Epoch 113/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1762 - mae: 0.3120 - val_loss: 0.1187 - val_mae: 0.2252 - learning_rate: 6.2500e-05\n",
            "Epoch 114/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1907 - mae: 0.3140 - val_loss: 0.1173 - val_mae: 0.2221 - learning_rate: 6.2500e-05\n",
            "Epoch 115/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1647 - mae: 0.2979 - val_loss: 0.1180 - val_mae: 0.2233 - learning_rate: 6.2500e-05\n",
            "Epoch 116/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1749 - mae: 0.3091 - val_loss: 0.1164 - val_mae: 0.2212 - learning_rate: 6.2500e-05\n",
            "Epoch 117/200\n",
            "\u001b[1m181/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1791 - mae: 0.3187\n",
            "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1792 - mae: 0.3186 - val_loss: 0.1162 - val_mae: 0.2212 - learning_rate: 6.2500e-05\n",
            "Epoch 118/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1657 - mae: 0.2926 - val_loss: 0.1152 - val_mae: 0.2199 - learning_rate: 3.1250e-05\n",
            "Epoch 119/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1899 - mae: 0.3215 - val_loss: 0.1150 - val_mae: 0.2194 - learning_rate: 3.1250e-05\n",
            "Epoch 120/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1841 - mae: 0.3155 - val_loss: 0.1159 - val_mae: 0.2203 - learning_rate: 3.1250e-05\n",
            "Epoch 121/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1844 - mae: 0.3172 - val_loss: 0.1167 - val_mae: 0.2201 - learning_rate: 3.1250e-05\n",
            "Epoch 122/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1819 - mae: 0.3127 - val_loss: 0.1168 - val_mae: 0.2214 - learning_rate: 3.1250e-05\n",
            "Epoch 123/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1923 - mae: 0.3242 - val_loss: 0.1165 - val_mae: 0.2208 - learning_rate: 3.1250e-05\n",
            "Epoch 124/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1916 - mae: 0.3252 - val_loss: 0.1170 - val_mae: 0.2213 - learning_rate: 3.1250e-05\n",
            "Epoch 125/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1878 - mae: 0.3216 - val_loss: 0.1163 - val_mae: 0.2195 - learning_rate: 3.1250e-05\n",
            "Epoch 126/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1778 - mae: 0.3088 - val_loss: 0.1165 - val_mae: 0.2198 - learning_rate: 3.1250e-05\n",
            "Epoch 127/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.1775 - mae: 0.3137 - val_loss: 0.1160 - val_mae: 0.2191 - learning_rate: 3.1250e-05\n",
            "Epoch 128/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1775 - mae: 0.3082 - val_loss: 0.1167 - val_mae: 0.2212 - learning_rate: 3.1250e-05\n",
            "Epoch 129/200\n",
            "\u001b[1m180/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1924 - mae: 0.3177\n",
            "Epoch 129: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1917 - mae: 0.3172 - val_loss: 0.1165 - val_mae: 0.2204 - learning_rate: 3.1250e-05\n",
            "Epoch 130/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1839 - mae: 0.3189 - val_loss: 0.1162 - val_mae: 0.2203 - learning_rate: 1.5625e-05\n",
            "Epoch 131/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1825 - mae: 0.3180 - val_loss: 0.1160 - val_mae: 0.2201 - learning_rate: 1.5625e-05\n",
            "Epoch 132/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1839 - mae: 0.3214 - val_loss: 0.1161 - val_mae: 0.2201 - learning_rate: 1.5625e-05\n",
            "Epoch 133/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1884 - mae: 0.3180 - val_loss: 0.1162 - val_mae: 0.2204 - learning_rate: 1.5625e-05\n",
            "Epoch 134/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1835 - mae: 0.3129 - val_loss: 0.1160 - val_mae: 0.2203 - learning_rate: 1.5625e-05\n",
            "Epoch 135/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1672 - mae: 0.3017 - val_loss: 0.1160 - val_mae: 0.2201 - learning_rate: 1.5625e-05\n",
            "Epoch 136/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1640 - mae: 0.2947 - val_loss: 0.1161 - val_mae: 0.2199 - learning_rate: 1.5625e-05\n",
            "Epoch 137/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1733 - mae: 0.3058 - val_loss: 0.1161 - val_mae: 0.2208 - learning_rate: 1.5625e-05\n",
            "Epoch 138/200\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1774 - mae: 0.3116 - val_loss: 0.1160 - val_mae: 0.2201 - learning_rate: 1.5625e-05\n",
            "Epoch 139/200\n",
            "\u001b[1m185/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1940 - mae: 0.3269\n",
            "Epoch 139: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1939 - mae: 0.3267 - val_loss: 0.1157 - val_mae: 0.2194 - learning_rate: 1.5625e-05\n",
            "Epoch 139: early stopping\n",
            "Restoring model weights from the end of the best epoch: 119.\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Performance Metrics:\n",
            "Root Mean Squared Error (RMSE): 0.3474\n",
            "Mean Absolute Error (MAE): 0.2160\n",
            "R2 Score: 0.9437\n",
            "\n",
            "====================================================================================================\n",
            "                                   STEP 5: RESULTS VISUALIZATION                                    \n",
            "                        Creating visual representations of model performance                        \n",
            "====================================================================================================\n",
            "\n",
            "--------------------------------------------------\n",
            "MODEL ARCHITECTURE AND FORMULAS\n",
            "Detailed explanation of model components and formulas\n",
            "--------------------------------------------------\n",
            "\n",
            "1. Linear Regression Model:\n",
            "--------------------------------------------------------------------------------\n",
            "Formula: y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε\n",
            "Where:\n",
            "  - y: Target variable (CO(GT))\n",
            "  - β₀: Intercept\n",
            "  - β₁ to βₙ: Coefficients for each feature\n",
            "  - x₁ to xₙ: Input features\n",
            "  - ε: Error term\n",
            "\n",
            "Optimization: Minimize Mean Squared Error (MSE)\n",
            "MSE = (1/n) * Σ(yᵢ - ŷᵢ)²\n",
            "Where:\n",
            "  - n: Number of samples\n",
            "  - yᵢ: Actual value\n",
            "  - ŷᵢ: Predicted value\n",
            "\n",
            "2. Random Forest Model:\n",
            "--------------------------------------------------------------------------------\n",
            "Ensemble of Decision Trees:\n",
            "Each tree prediction: ŷ = f(x₁, x₂, ..., xₙ)\n",
            "Final prediction: ŷ = (1/T) * Σ ŷᵢ\n",
            "Where:\n",
            "  - T: Number of trees (n_estimators=100)\n",
            "  - ŷᵢ: Prediction from i-th tree\n",
            "\n",
            "Feature Importance Calculation:\n",
            "Importance = (1/T) * Σ (node_impurity - left_impurity - right_impurity)\n",
            "Where:\n",
            "  - node_impurity: Gini impurity of the node\n",
            "  - left_impurity: Gini impurity of left child\n",
            "  - right_impurity: Gini impurity of right child\n",
            "\n",
            "3. Neural Network Model:\n",
            "--------------------------------------------------------------------------------\n",
            "Architecture:\n",
            "Input Layer (16 features) → Dense(256) → BatchNorm → Dropout(0.4)\n",
            "→ Dense(128) → BatchNorm → Dropout(0.3) → Dense(256) → BatchNorm\n",
            "→ Dense(64) → BatchNorm → Dropout(0.2) → Dense(256) → BatchNorm\n",
            "→ Dense(32) → BatchNorm → Output(1)\n",
            "\n",
            "Activation Functions:\n",
            "ReLU: f(x) = max(0, x)\n",
            "Linear: f(x) = x (output layer)\n",
            "\n",
            "Batch Normalization:\n",
            "x̂ = (x - μ) / √(σ² + ε)\n",
            "y = γx̂ + β\n",
            "Where:\n",
            "  - μ: Mean of batch\n",
            "  - σ²: Variance of batch\n",
            "  - ε: Small constant\n",
            "  - γ: Scale parameter\n",
            "  - β: Shift parameter\n",
            "\n",
            "Dropout:\n",
            "During training: x' = x * m\n",
            "During inference: x' = x * p\n",
            "Where:\n",
            "  - m: Binary mask (Bernoulli distribution)\n",
            "  - p: Dropout probability\n",
            "\n",
            "4. Performance Metrics:\n",
            "--------------------------------------------------------------------------------\n",
            "Root Mean Squared Error (RMSE):\n",
            "RMSE = √(MSE) = √[(1/n) * Σ(yᵢ - ŷᵢ)²]\n",
            "\n",
            "Mean Absolute Error (MAE):\n",
            "MAE = (1/n) * Σ|yᵢ - ŷᵢ|\n",
            "\n",
            "R² Score (Coefficient of Determination):\n",
            "R² = 1 - (Σ(yᵢ - ŷᵢ)² / Σ(yᵢ - ȳ)²)\n",
            "Where:\n",
            "  - ȳ: Mean of actual values\n",
            "\n",
            "5. Error Distribution Metrics:\n",
            "--------------------------------------------------------------------------------\n",
            "Skewness:\n",
            "γ₁ = (1/n) * Σ[(xᵢ - μ)/σ]³\n",
            "Where:\n",
            "  - μ: Mean of errors\n",
            "  - σ: Standard deviation of errors\n",
            "\n",
            "Kurtosis:\n",
            "γ₂ = (1/n) * Σ[(xᵢ - μ)/σ]⁴ - 3\n",
            "Where:\n",
            "  - μ: Mean of errors\n",
            "  - σ: Standard deviation of errors\n",
            "\n",
            "--------------------------------------------------\n",
            "5.1 Model Performance Comparison\n",
            "Comparing predictions across all models\n",
            "--------------------------------------------------\n",
            "\n",
            "Detailed Model Performance Statistics:\n",
            "--------------------------------------------------------------------------------\n",
            "Model                RMSE       MAE        R2         Max Error    Min Error   \n",
            "--------------------------------------------------------------------------------\n",
            "Linear Regression    0.4125 0.2603 0.9206 3.1335 -3.3160\n",
            "Random Forest        0.3481 0.2135 0.9435 2.1826 -2.8960\n",
            "Neural Network       0.3474 0.2160 0.9437 2.6354 -2.6945\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--------------------------------------------------\n",
            "5.2 Feature Importance Analysis\n",
            "Analyzing feature importance in Random Forest model\n",
            "--------------------------------------------------\n",
            "\n",
            "Top 5 Most Important Features:\n",
            "--------------------------------------------------\n",
            "1. C6H6(GT)             0.5183\n",
            "2. PT08.S2(NMHC)        0.3278\n",
            "3. NOx(GT)              0.0371\n",
            "4. NO2(GT)              0.0242\n",
            "5. PT08.S1(CO)          0.0202\n",
            "--------------------------------------------------\n",
            "\n",
            "--------------------------------------------------\n",
            "5.3 Error Distribution Analysis\n",
            "Analyzing error distribution across models\n",
            "--------------------------------------------------\n",
            "\n",
            "Error Distribution Statistics:\n",
            "--------------------------------------------------------------------------------\n",
            "Model                Mean Error   Std Error    Skewness     Kurtosis    \n",
            "--------------------------------------------------------------------------------\n",
            "Linear Regression    0.0038 0.4126 -0.4561 10.1941\n",
            "Random Forest        0.0012 0.3482 -0.4111 11.1939\n",
            "Neural Network       -0.0161 0.3471 -0.7742 11.1002\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--------------------------------------------------\n",
            "5.4 Model Performance Metrics\n",
            "Comparing model performance metrics\n",
            "--------------------------------------------------\n",
            "\n",
            "Detailed Model Performance Metrics:\n",
            "--------------------------------------------------------------------------------\n",
            "            Model     RMSE      MAE       R2\n",
            "Linear Regression 0.412463 0.260286 0.920631\n",
            "    Random Forest 0.348079 0.213534 0.943476\n",
            "   Neural Network 0.347428 0.215968 0.943687\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Visualization Summary:\n",
            "================================================================================\n",
            "1. Model Performance Comparison (model_performance.png):\n",
            "   - Actual vs Predicted scatter plots for all models\n",
            "   - Neural Network training history\n",
            "   - R2 scores for each model\n",
            "   - Formulas used for each model's predictions\n",
            "\n",
            "2. Feature Importance Analysis (feature_importance.png):\n",
            "   - Bar plot of feature importance from Random Forest\n",
            "   - Top 5 most important features identified\n",
            "   - Feature importance calculation formula\n",
            "\n",
            "3. Error Distribution Analysis (error_distribution.png):\n",
            "   - Histograms of prediction errors for each model\n",
            "   - Error statistics including mean, std, skewness, and kurtosis\n",
            "   - Error distribution formulas and calculations\n",
            "\n",
            "4. Model Performance Metrics (metrics_comparison.png):\n",
            "   - Comparison of RMSE, MAE, and R2 across all models\n",
            "   - Best performing model for each metric\n",
            "   - Detailed formulas for each metric\n",
            "================================================================================\n",
            "\n",
            "All visualizations have been saved in the 'plots' directory.\n",
            "You can find detailed performance metrics, formulas, and analysis in the plots.\n",
            "\n",
            "====================================================================================================\n",
            "                                           SAVING MODELS                                            \n",
            "                      Saving trained models and scaler for future predictions                       \n",
            "====================================================================================================\n",
            "\n",
            "Models and scaler saved successfully!\n",
            "\n",
            "====================================================================================================\n",
            "                                         PROJECT COMPLETED                                          \n",
            "                      All models have been trained and evaluated successfully                       \n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict.py"
      ],
      "metadata": {
        "id": "LbkfMvRJl_z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"Load all trained models and scaler\"\"\"\n",
        "    try:\n",
        "        # Load models from the models directory\n",
        "        linear_model = joblib.load('models/linear_model.joblib')\n",
        "        rf_model = joblib.load('models/rf_model.joblib')\n",
        "        nn_model = tf.keras.models.load_model('models/nn_model.keras')\n",
        "        scaler = joblib.load('models/scaler.joblib')\n",
        "        return linear_model, rf_model, nn_model, scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def validate_input(feature, value):\n",
        "    \"\"\"Validate input values against predefined ranges\"\"\"\n",
        "    ranges = {\n",
        "        'T': (-50, 50),        # Temperature range in Celsius\n",
        "        'RH': (0, 100),        # Relative Humidity range\n",
        "        'Hour': (0, 23),       # Hour of day\n",
        "        'NOx(GT)': (0, 1000),  # NOx range\n",
        "        'NO2(GT)': (0, 1000),  # NO2 range\n",
        "        'NMHC(GT)': (0, 1000), # NMHC range\n",
        "        'C6H6(GT)': (0, 100)   # Benzene range\n",
        "    }\n",
        "\n",
        "    if feature in ranges:\n",
        "        min_val, max_val = ranges[feature]\n",
        "        if not (min_val <= value <= max_val):\n",
        "            raise ValueError(f\"{feature} must be between {min_val} and {max_val}\")\n",
        "    return True\n",
        "\n",
        "def get_user_input():\n",
        "    \"\"\"Get input values from user with validation\"\"\"\n",
        "    print(\"\\nEnter the following air quality measurements:\")\n",
        "\n",
        "    features = {\n",
        "        'PT08.S1(CO)': 'CO sensor (tin oxide) response',\n",
        "        'PT08.S2(NMHC)': 'NMHC sensor (titania) response',\n",
        "        'PT08.S3(NOx)': 'NOx sensor (tungsten oxide) response',\n",
        "        'PT08.S4(NO2)': 'NO2 sensor (tungsten oxide) response',\n",
        "        'PT08.S5(O3)': 'Ozone sensor (indium oxide) response',\n",
        "        'T': 'Temperature in °C',\n",
        "        'RH': 'Relative Humidity (%)',\n",
        "        'C6H6(GT)': 'Benzene concentration in µg/m³',\n",
        "        'NOx(GT)': 'NOx concentration in ppb',\n",
        "        'NO2(GT)': 'NO2 concentration in µg/m³',\n",
        "        'NMHC(GT)': 'Non Metanic HydroCarbons concentration in µg/m³'\n",
        "    }\n",
        "\n",
        "    input_data = {}\n",
        "    for feature, description in features.items():\n",
        "        while True:\n",
        "            try:\n",
        "                value = float(input(f\"{description}: \"))\n",
        "                validate_input(feature, value)\n",
        "                input_data[feature] = value\n",
        "                break\n",
        "            except ValueError as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                print(\"Please enter a valid number\")\n",
        "\n",
        "    # Add current time-based features\n",
        "    current_time = datetime.now()\n",
        "    input_data['Hour'] = current_time.hour\n",
        "    input_data['Month'] = current_time.month\n",
        "    input_data['DayOfYear'] = current_time.timetuple().tm_yday\n",
        "\n",
        "    # Calculate derived features\n",
        "    input_data['NOx_NO2_ratio'] = input_data['NOx(GT)'] / (input_data['NO2(GT)'] + 1e-6)\n",
        "    input_data['CO_NMHC_ratio'] = input_data['PT08.S1(CO)'] / (input_data['NMHC(GT)'] + 1e-6)\n",
        "\n",
        "    # Create DataFrame with correct feature order\n",
        "    feature_order = [\n",
        "        'PT08.S1(CO)', 'PT08.S2(NMHC)', 'PT08.S3(NOx)', 'PT08.S4(NO2)',\n",
        "        'PT08.S5(O3)', 'T', 'RH', 'C6H6(GT)', 'NOx(GT)', 'NO2(GT)', 'NMHC(GT)',\n",
        "        'Hour', 'Month', 'DayOfYear', 'NOx_NO2_ratio', 'CO_NMHC_ratio'\n",
        "    ]\n",
        "\n",
        "    return pd.DataFrame([input_data])[feature_order]\n",
        "\n",
        "def make_predictions(input_data, models, scaler):\n",
        "    \"\"\"Make predictions using all models with error handling\"\"\"\n",
        "    try:\n",
        "        # Scale the input data\n",
        "        scaled_data = scaler.transform(input_data)\n",
        "\n",
        "        # Make predictions\n",
        "        linear_pred = models[0].predict(scaled_data)[0]\n",
        "        rf_pred = models[1].predict(scaled_data)[0]\n",
        "        nn_pred = models[2].predict(scaled_data, verbose=0)[0][0]\n",
        "\n",
        "        return {\n",
        "            'Linear Regression': linear_pred,\n",
        "            'Random Forest': rf_pred,\n",
        "            'Neural Network': nn_pred\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error making predictions: {e}\")\n",
        "        return None\n",
        "\n",
        "def display_results(predictions):\n",
        "    \"\"\"Display prediction results with air quality categories\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CO Level Predictions:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if predictions is None:\n",
        "        print(\"Unable to make predictions due to an error\")\n",
        "        return\n",
        "\n",
        "    for model, pred in predictions.items():\n",
        "        print(f\"{model}: {pred:.2f} mg/m³\")\n",
        "\n",
        "    # Calculate average prediction\n",
        "    avg_pred = sum(predictions.values()) / len(predictions)\n",
        "    print(\"\\nEnsemble Average Prediction:\", f\"{avg_pred:.2f} mg/m³\")\n",
        "\n",
        "    # Display air quality category\n",
        "    print(\"\\nAir Quality Category:\")\n",
        "    if avg_pred < 1:\n",
        "        print(\"Good: Air quality is considered satisfactory\")\n",
        "        print(\"Health implications: Little to no risk\")\n",
        "    elif avg_pred < 2:\n",
        "        print(\"Moderate: Air quality is acceptable\")\n",
        "        print(\"Health implications: Some pollutants may affect very sensitive individuals\")\n",
        "    elif avg_pred < 3:\n",
        "        print(\"Unhealthy for Sensitive Groups\")\n",
        "        print(\"Health implications: Children and people with respiratory conditions may experience health effects\")\n",
        "    elif avg_pred < 4:\n",
        "        print(\"Unhealthy\")\n",
        "        print(\"Health implications: Everyone may begin to experience health effects\")\n",
        "    elif avg_pred < 5:\n",
        "        print(\"Very Unhealthy\")\n",
        "        print(\"Health implications: Health warnings of emergency conditions\")\n",
        "    else:\n",
        "        print(\"Hazardous: Health warnings of emergency conditions\")\n",
        "        print(\"Health implications: Everyone is more likely to be affected\")\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*50)\n",
        "    print(\"Air Quality Index Prediction System\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load models\n",
        "    print(\"\\nLoading trained models...\")\n",
        "    models = load_models()\n",
        "\n",
        "    if None in models:\n",
        "        print(\"Error: Could not load all required models.\")\n",
        "        print(\"Please ensure all model files exist in the 'models' directory.\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Get user input\n",
        "            input_data = get_user_input()\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = make_predictions(input_data, models, models[3])\n",
        "\n",
        "            # Display results\n",
        "            display_results(predictions)\n",
        "\n",
        "            # Ask if user wants to make another prediction\n",
        "            while True:\n",
        "                another = input(\"\\nWould you like to make another prediction? (y/n): \").lower()\n",
        "                if another in ['y', 'n']:\n",
        "                    break\n",
        "                print(\"Please enter 'y' or 'n'\")\n",
        "\n",
        "            if another == 'n':\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred: {e}\")\n",
        "            print(\"Please try again.\")\n",
        "\n",
        "    print(\"\\nThank you for using the Air Quality Index Prediction System!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzxORtYgmBE8",
        "outputId": "5c9d9b7a-581f-4c8f-be0c-0f6ac69e59d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Air Quality Index Prediction System\n",
            "==================================================\n",
            "\n",
            "Loading trained models...\n",
            "\n",
            "Enter the following air quality measurements:\n",
            "CO sensor (tin oxide) response: 1\n",
            "NMHC sensor (titania) response: 1\n",
            "NOx sensor (tungsten oxide) response: 1\n",
            "NO2 sensor (tungsten oxide) response: 1\n",
            "Ozone sensor (indium oxide) response: 1\n",
            "Temperature in °C: 1\n",
            "Relative Humidity (%): 1\n",
            "Benzene concentration in µg/m³: 1\n",
            "NOx concentration in ppb: 1\n",
            "NO2 concentration in µg/m³: 1\n",
            "Non Metanic HydroCarbons concentration in µg/m³: 1\n",
            "\n",
            "==================================================\n",
            "CO Level Predictions:\n",
            "==================================================\n",
            "Linear Regression: -0.40 mg/m³\n",
            "Random Forest: 0.49 mg/m³\n",
            "Neural Network: 0.63 mg/m³\n",
            "\n",
            "Ensemble Average Prediction: 0.24 mg/m³\n",
            "\n",
            "Air Quality Category:\n",
            "Good: Air quality is considered satisfactory\n",
            "Health implications: Little to no risk\n",
            "\n",
            "Would you like to make another prediction? (y/n): y\n",
            "\n",
            "Enter the following air quality measurements:\n",
            "CO sensor (tin oxide) response: 1360\n",
            "NMHC sensor (titania) response: 1046\n",
            "NOx sensor (tungsten oxide) response: 1056\n",
            "NO2 sensor (tungsten oxide) response: 1692\n",
            "Ozone sensor (indium oxide) response: 1268\n",
            "Temperature in °C: 13.6\n",
            "Relative Humidity (%): 48.9\n",
            "Benzene concentration in µg/m³: 11.9\n",
            "NOx concentration in ppb: 166\n",
            "NO2 concentration in µg/m³: 113\n",
            "Non Metanic HydroCarbons concentration in µg/m³: 150\n",
            "\n",
            "==================================================\n",
            "CO Level Predictions:\n",
            "==================================================\n",
            "Linear Regression: 2.81 mg/m³\n",
            "Random Forest: 2.79 mg/m³\n",
            "Neural Network: 2.70 mg/m³\n",
            "\n",
            "Ensemble Average Prediction: 2.77 mg/m³\n",
            "\n",
            "Air Quality Category:\n",
            "Unhealthy for Sensitive Groups\n",
            "Health implications: Children and people with respiratory conditions may experience health effects\n",
            "\n",
            "Would you like to make another prediction? (y/n): n\n",
            "\n",
            "Thank you for using the Air Quality Index Prediction System!\n"
          ]
        }
      ]
    }
  ]
}